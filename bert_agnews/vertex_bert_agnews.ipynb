{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fdf3b6-fe71-4034-9163-d3d87aedf9b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T22:04:13.423918Z",
     "iopub.status.busy": "2025-11-27T22:04:13.422558Z",
     "iopub.status.idle": "2025-11-27T22:04:13.436344Z",
     "shell.execute_reply": "2025-11-27T22:04:13.434929Z",
     "shell.execute_reply.started": "2025-11-27T22:04:13.423858Z"
    },
    "tags": []
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
    "3. **Enable the required Google Cloud APIs**\n",
    "\n",
    "This tutorial uses a **custom prediction container**, **Artifact Registry**, **Cloud Build**, and **Vertex AI**, so you must enable all the following:\n",
    "\n",
    "| API | Purpose |\n",
    "|-----|---------|\n",
    "| `aiplatform.googleapis.com` | Vertex AI (model registry, endpoints, predictions) |\n",
    "| `artifactregistry.googleapis.com` | Stores your custom Docker images |\n",
    "| `cloudbuild.googleapis.com` | Builds the container image from your Dockerfile |\n",
    "| `storage.googleapis.com` | Allows reading/writing to Cloud Storage buckets |\n",
    "| `compute.googleapis.com` | Required for Endpoint deployment VMs |\n",
    "\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk). [How to Install Cloud SDK on macOS](https://medium.com/@philipsatimor2/how-to-install-and-configure-google-cloud-sdk-on-macos-using-homebrew-cc41f36dc592)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acdc2c-09fb-49cc-9c14-1a07df3588e2",
   "metadata": {},
   "source": [
    "# Practical MLOps with Vertex AI: Deploying ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f5462-cb84-4232-b97b-91581947b5ee",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This example demonstrates how to build, package, and deploy a custom machine learning model using **Vertex AI**. Unlike AutoML or prebuilt containers, this workflow shows how to take a **PyTorch model you trained yourself**, wrap it in a **FastAPI prediction server**, containerize it with **Docker**, and deploy it as a **fully managed, scalable endpoint** on Google Cloud.\n",
    "\n",
    "You will walk through the complete MLOps lifecycle:\n",
    "\n",
    "- Training a BERT text-classification model on a small dataset  \n",
    "- Saving the model artifacts needed for inference (`model.pt`)  \n",
    "- Creating custom inference code (`inference.py`)  \n",
    "- Building a prediction API with FastAPI (`server.py`)  \n",
    "- Creating a lightweight Docker image  \n",
    "- Using **Cloud Build** to build and push the image  \n",
    "- Registering and deploying the model with **Vertex AI**  \n",
    "- Sending live prediction requests to the deployed endpoint  \n",
    "\n",
    "This example is ideal for:\n",
    "\n",
    "- ML engineers who want full control over their serving stack  \n",
    "- Data scientists who need more flexibility than AutoML provides  \n",
    "- Developers learning modern MLOps practices on Google Cloud  \n",
    "- Anyone deploying **PyTorch models** using custom logic  \n",
    "\n",
    "Before you begin, you should have:\n",
    "\n",
    "- Basic familiarity with Python and PyTorch  \n",
    "- A working understanding of virtual environments or notebooks  \n",
    "- A Google Cloud project with required APIs enabled  \n",
    "- Very basic knowledge of Docker (enough to understand a Dockerfile)  \n",
    "\n",
    "Learn more about [Vertex AI custom model deployment](https://cloud.google.com/vertex-ai/docs/predictions/custom-models).  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Project Setup and Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7436ea0-24ca-46db-a701-7a5fcb9703fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:11:41.082458Z",
     "iopub.status.busy": "2025-11-27T13:11:41.082033Z",
     "iopub.status.idle": "2025-11-27T13:11:44.001601Z",
     "shell.execute_reply": "2025-11-27T13:11:43.999776Z",
     "shell.execute_reply.started": "2025-11-27T13:11:41.082421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e41d239-4cec-46e7-a7a7-c3b053b918a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:11:46.450405Z",
     "iopub.status.busy": "2025-11-27T13:11:46.450003Z",
     "iopub.status.idle": "2025-11-27T13:11:47.605679Z",
     "shell.execute_reply": "2025-11-27T13:11:47.603613Z",
     "shell.execute_reply.started": "2025-11-27T13:11:46.450365Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFORMATION: Project 'vast-collective-478617-j1' has no 'environment' tag set. Use either 'Production', 'Development', 'Test', or 'Staging'. Add an 'environment' tag using `gcloud resource-manager tags bindings create`.\n",
      "[compute]\n",
      "region = us-central1\n",
      "[core]\n",
      "account = 60487384516-compute@developer.gserviceaccount.com\n",
      "disable_usage_reporting = True\n",
      "project = vast-collective-478617-j1\n",
      "universe_domain = googleapis.com\n",
      "[dataproc]\n",
      "region = us-central1\n",
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fcf69-f8f4-4844-b882-eb226a9cf411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment setup and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39be4ae8-a44f-4ea9-a0ed-89f91f171ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:11:51.538966Z",
     "iopub.status.busy": "2025-11-27T13:11:51.538507Z",
     "iopub.status.idle": "2025-11-27T13:11:51.546915Z",
     "shell.execute_reply": "2025-11-27T13:11:51.545214Z",
     "shell.execute_reply.started": "2025-11-27T13:11:51.538926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vast-collective-478617-j1 us-central1 vertex-mlops-philip-devfest\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"vast-collective-478617-j1\"       # Your GCP project ID\n",
    "REGION = \"us-central1\"                         # Vertex AI region (must match resources)\n",
    "BUCKET_NAME = \"vertex-mlops-philip-devfest\"    # Existing GCS bucket\n",
    "MODEL_DIR = \"bert_agnews_model\"                # Local folder to store model artifacts\n",
    "ENDPOINT_DISPLAY_NAME = \"bert-agnews-endpoint\"\n",
    "\n",
    "print(PROJECT_ID, REGION, BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f261035-feda-4f9a-9c3a-812275579892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:11:58.337288Z",
     "iopub.status.busy": "2025-11-27T13:11:58.336663Z",
     "iopub.status.idle": "2025-11-27T13:11:58.343986Z",
     "shell.execute_reply": "2025-11-27T13:11:58.342649Z",
     "shell.execute_reply.started": "2025-11-27T13:11:58.337242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_REPO = \"vertex-mlops\"              # Artifact Registry repo name (will be created if needed)\n",
    "IMAGE_NAME = \"bert-agnews-workshop\"      #Image name\n",
    "IMAGE_TAG =\"v1\"                          #Image tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4787441b-d046-407d-b776-14d458e2f87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:12:02.570995Z",
     "iopub.status.busy": "2025-11-27T13:12:02.569450Z",
     "iopub.status.idle": "2025-11-27T13:12:02.577230Z",
     "shell.execute_reply": "2025-11-27T13:12:02.575775Z",
     "shell.execute_reply.started": "2025-11-27T13:12:02.570942Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPO}/{IMAGE_NAME}:{IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121b37eb-0592-4ff2-8225-575fdaf05033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:12:06.451344Z",
     "iopub.status.busy": "2025-11-27T13:12:06.450909Z",
     "iopub.status.idle": "2025-11-27T13:12:06.458204Z",
     "shell.execute_reply": "2025-11-27T13:12:06.456830Z",
     "shell.execute_reply.started": "2025-11-27T13:12:06.451297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: vast-collective-478617-j1\n",
      "Region: us-central1\n",
      "Bucket: vertex-mlops-philip-devfest\n",
      "Image URI: us-central1-docker.pkg.dev/vast-collective-478617-j1/vertex-mlops/bert-agnews-workshop:v1\n"
     ]
    }
   ],
   "source": [
    "print(\"Project:\", PROJECT_ID)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket:\", BUCKET_NAME)\n",
    "print(\"Image URI:\", IMAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1463a137-c941-4393-99f9-c486e1535c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:12:10.167714Z",
     "iopub.status.busy": "2025-11-27T13:12:10.166252Z",
     "iopub.status.idle": "2025-11-27T13:12:11.351490Z",
     "shell.execute_reply": "2025-11-27T13:12:11.349932Z",
     "shell.execute_reply.started": "2025-11-27T13:12:10.167653Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFORMATION: Project 'vast-collective-478617-j1' has no 'environment' tag set. Use either 'Production', 'Development', 'Test', or 'Staging'. Add an 'environment' tag using `gcloud resource-manager tags bindings create`.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {PROJECT_ID}   # Set the active project for gcloud and SDK tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ff810-16bf-45c4-afbc-0aafc7399b08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:11:01.088985Z",
     "iopub.status.busy": "2025-11-26T18:11:01.087404Z",
     "iopub.status.idle": "2025-11-26T18:11:04.046466Z",
     "shell.execute_reply": "2025-11-26T18:11:04.044585Z",
     "shell.execute_reply.started": "2025-11-26T18:11:01.088930Z"
    },
    "tags": []
   },
   "source": [
    "### 2.3. Install Python dependencies (for training and local testing)\n",
    "\n",
    "We install training and serving libraries into our notebook environment.\n",
    "\n",
    "- `transformers`, `datasets`, `torch` for model + data  \n",
    "- `scikit-learn` for evaluation  \n",
    "- `google-cloud-aiplatform` to interact with Vertex AI  \n",
    "- `fastapi`, `uvicorn` for our custom prediction container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09e388f-8f89-4a91-a616-3ff5dc6d6e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:12:38.513790Z",
     "iopub.status.busy": "2025-11-27T13:12:38.513311Z",
     "iopub.status.idle": "2025-11-27T13:12:41.301718Z",
     "shell.execute_reply": "2025-11-27T13:12:41.300064Z",
     "shell.execute_reply.started": "2025-11-27T13:12:38.513744Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch scikit-learn pandas numpy google-cloud-aiplatform fastapi uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2701f9f5-edd5-43db-8bda-6736432cfc31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:11:39.371951Z",
     "iopub.status.busy": "2025-11-26T18:11:39.370956Z",
     "iopub.status.idle": "2025-11-26T18:11:39.377673Z",
     "shell.execute_reply": "2025-11-26T18:11:39.376376Z",
     "shell.execute_reply.started": "2025-11-26T18:11:39.371903Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Re-start kernel if needed after install."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49eeda-9857-43be-afd1-7f2fae221bba",
   "metadata": {},
   "source": [
    "#### 2.4. Initialize the Vertex AI Python SDK\n",
    "\n",
    "This lets us use the high-level Vertex AI client in code instead of clicking in the UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b69e2b60-bce6-4c28-bf38-8004acef52d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:12:56.046145Z",
     "iopub.status.busy": "2025-11-27T13:12:56.045649Z",
     "iopub.status.idle": "2025-11-27T13:12:58.556348Z",
     "shell.execute_reply": "2025-11-27T13:12:58.555222Z",
     "shell.execute_reply.started": "2025-11-27T13:12:56.046104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=f\"gs://{BUCKET_NAME}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3c0ec-a600-40c9-ad99-e0146e373e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a Small Sample of the AG News Dataset (for fast ..considering time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4837b07-5f76-4aa8-aeea-914a3541239a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T14:55:02.898859Z",
     "iopub.status.busy": "2025-11-27T14:55:02.898286Z",
     "iopub.status.idle": "2025-11-27T14:55:02.908464Z",
     "shell.execute_reply": "2025-11-27T14:55:02.907138Z",
     "shell.execute_reply.started": "2025-11-27T14:55:02.898805Z"
    },
    "tags": []
   },
   "source": [
    "#### 3. Load and Sample the AG News Dataset\n",
    "\n",
    "We use the Hugging Face `datasets` library to load AG News and then take a **tiny subset** to keep training fast.\n",
    "\n",
    "- 200 examples for training  \n",
    "- 50 examples for validation  \n",
    "- 100 examples for testing  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fd4f3-f6ce-4b7c-9f60-dd45edd2352e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the [AG News](https://huggingface.co/datasets/ag_news) dataset, a benchmark dataset for text classification with four categories:\n",
    "\n",
    "| Label | Category |\n",
    "| ----- | -------- |\n",
    "| 0     | World    |\n",
    "| 1     | Sports   |\n",
    "| 2     | Business |\n",
    "| 3     | Sci/Tech |\n",
    "\n",
    "The dataset is tokenized using the `BertTokenizer` and then mapped into a format suitable for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf66955-9b0e-4359-918a-6ef7e7747760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:27:46.369897Z",
     "iopub.status.busy": "2025-11-26T18:27:46.369409Z",
     "iopub.status.idle": "2025-11-26T18:27:47.263746Z",
     "shell.execute_reply": "2025-11-26T18:27:47.262323Z",
     "shell.execute_reply.started": "2025-11-26T18:27:46.369858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_full = dataset[\"train\"]\n",
    "test_full = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e37ec7c3-be86-40bd-ab8d-d0f053b9a3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:28:54.588990Z",
     "iopub.status.busy": "2025-11-26T18:28:54.588159Z",
     "iopub.status.idle": "2025-11-26T18:28:54.595602Z",
     "shell.execute_reply": "2025-11-26T18:28:54.594066Z",
     "shell.execute_reply.started": "2025-11-26T18:28:54.588947Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 7600\n"
     ]
    }
   ],
   "source": [
    "print( len(train_full), len(test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd776168-addb-4f15-ae54-a7fda703e08e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:29:27.666372Z",
     "iopub.status.busy": "2025-11-26T18:29:27.665973Z",
     "iopub.status.idle": "2025-11-26T18:29:27.725364Z",
     "shell.execute_reply": "2025-11-26T18:29:27.723925Z",
     "shell.execute_reply.started": "2025-11-26T18:29:27.666340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TRAIN = 200\n",
    "N_VAL = 50\n",
    "N_TEST = 100\n",
    "\n",
    "small_train = train_full.shuffle(seed=42).select(range(N_TRAIN + N_VAL))\n",
    "small_test = test_full.shuffle(seed=42).select(range(N_TEST))\n",
    "\n",
    "train_ds = small_train.select(range(N_TRAIN))\n",
    "val_ds = small_train.select(range(N_TRAIN, N_TRAIN + N_VAL))\n",
    "\n",
    "len(train_ds), len(val_ds), len(small_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc45c2-192f-441d-a129-9d2c8ee9dfb4",
   "metadata": {},
   "source": [
    "#### 4. Tokenize Text with DistilBERT\n",
    "\n",
    "We convert raw text into token IDs required by the DistilBERT model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load the DistilBERT tokenizer.  \n",
    "2. Define a function to tokenize and pad/truncate.  \n",
    "3. Apply it to train/val/test splits.  \n",
    "4. Configure them to output PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c8131e-0636-4cb5-97b8-3138997fb250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:30:20.294548Z",
     "iopub.status.busy": "2025-11-26T18:30:20.294116Z",
     "iopub.status.idle": "2025-11-26T18:30:27.868659Z",
     "shell.execute_reply": "2025-11-26T18:30:27.867289Z",
     "shell.execute_reply.started": "2025-11-26T18:30:20.294510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51d69e070d34fb1ba571be487916b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc43e677f0974d958c158ba67492d189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaaac2305a84b67be8e51540bb0de78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e818264d4fd4ef68801d1752c0cd2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29445d0276d4aab8772698d1efa65c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bfa53ddce94f17918e747d7fec7b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f95d12596ff42fa98a15383ff2fc6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b55b74d5fc4bd69c989961b4b67212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_enc = train_ds.map(tokenize_batch, batched=True)\n",
    "val_enc = val_ds.map(tokenize_batch, batched=True)\n",
    "test_enc = small_test.map(tokenize_batch, batched=True)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "for ds in [train_enc, val_enc, test_enc]:\n",
    "    ds.set_format(type=\"torch\", columns=cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b089d-f994-4ef8-af74-7205bc461cc1",
   "metadata": {},
   "source": [
    "#### 5. Create PyTorch DataLoaders\n",
    "\n",
    "We wrap the tokenized datasets in DataLoaders:\n",
    "\n",
    "- Batches data  \n",
    "- Shuffles training set  \n",
    "- Keeps validation and test deterministic  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fbea863-1436-4d13-95e9-898ee40e74ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:31:36.576044Z",
     "iopub.status.busy": "2025-11-26T18:31:36.575602Z",
     "iopub.status.idle": "2025-11-26T18:31:36.583078Z",
     "shell.execute_reply": "2025-11-26T18:31:36.581616Z",
     "shell.execute_reply.started": "2025-11-26T18:31:36.576009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16   # small batch size for CPU-friendly training\n",
    "\n",
    "train_loader = DataLoader(train_enc, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_enc, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_enc, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12c3b9-9e1d-40a0-ac8e-7ccf0047d184",
   "metadata": {},
   "source": [
    "#### 6. Define the DistilBERT Classifier in PyTorch\n",
    "\n",
    "We build a simple classifier:\n",
    "\n",
    "- DistilBERT encoder  \n",
    "- Dropout regularization  \n",
    "- Linear layer for 4 AG News labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a411d22f-33e3-48cc-8cfd-01a8e1d71d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:32:42.920093Z",
     "iopub.status.busy": "2025-11-26T18:32:42.919680Z",
     "iopub.status.idle": "2025-11-26T18:32:42.945213Z",
     "shell.execute_reply": "2025-11-26T18:32:42.944026Z",
     "shell.execute_reply.started": "2025-11-26T18:32:42.920059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5769300-5571-4cbd-924e-6ecf9169e78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:32:55.943916Z",
     "iopub.status.busy": "2025-11-26T18:32:55.943511Z",
     "iopub.status.idle": "2025-11-26T18:32:55.950794Z",
     "shell.execute_reply": "2025-11-26T18:32:55.949456Z",
     "shell.execute_reply.started": "2025-11-26T18:32:55.943880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(base_model_name)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        x = self.dropout(pooled)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0664d26-c047-43ed-b7ce-ef1316f93fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:33:13.669202Z",
     "iopub.status.busy": "2025-11-26T18:33:13.668785Z",
     "iopub.status.idle": "2025-11-26T18:33:19.063771Z",
     "shell.execute_reply": "2025-11-26T18:33:19.062734Z",
     "shell.execute_reply.started": "2025-11-26T18:33:13.669166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513c05bde96349ecb19403b20193bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BertClassifier(MODEL_NAME).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec2619-c32f-4cae-9466-d014fb8192a7",
   "metadata": {},
   "source": [
    "#Training the model (FAST)\n",
    "#### 7. Train the Model (Fast)\n",
    "\n",
    "We train for **one epoch** on our tiny subset.\n",
    "\n",
    "The goal is not maximum accuracy, but to demonstrate the workflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f3f1a19-6aa0-40ca-b0c2-e9d3facca2cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:34:22.576309Z",
     "iopub.status.busy": "2025-11-26T18:34:22.575634Z",
     "iopub.status.idle": "2025-11-26T18:34:22.584673Z",
     "shell.execute_reply": "2025-11-26T18:34:22.583234Z",
     "shell.execute_reply.started": "2025-11-26T18:34:22.576272Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        # Compute loss between predicted logits and true labels\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        #update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_examples += labels.size(0)\n",
    "\n",
    "    return total_correct / total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff457446-5cb6-4f9e-840a-d9ed31c617de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:36:22.604280Z",
     "iopub.status.busy": "2025-11-26T18:36:22.603878Z",
     "iopub.status.idle": "2025-11-26T18:39:28.459957Z",
     "shell.execute_reply": "2025-11-26T18:39:28.458703Z",
     "shell.execute_reply.started": "2025-11-26T18:36:22.604243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a97186a3e4ded9c84026926599c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 accuracy: 0.805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98ac63cb893415097b137a19bfe93b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 accuracy: 0.915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfc60a827fc44219132a5e0c65ec2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 accuracy: 0.955\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3  # Fast for workshop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    acc = train_one_epoch(model, train_loader)\n",
    "    print(f\"Epoch {epoch+1} accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcb488-197e-40ed-a0d3-dab1e3bc1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55427398-e048-4af6-8680-ee7630211e1e",
   "metadata": {},
   "source": [
    "#### 8. Evaluate the Model\n",
    "\n",
    "We evaluate on the test set and compute:\n",
    "\n",
    "- Accuracy  \n",
    "- Confusion matrix  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4724870-e12b-43cf-a886-85ae1812eefd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:40:21.584295Z",
     "iopub.status.busy": "2025-11-26T18:40:21.583486Z",
     "iopub.status.idle": "2025-11-26T18:40:30.711836Z",
     "shell.execute_reply": "2025-11-26T18:40:30.710624Z",
     "shell.execute_reply.started": "2025-11-26T18:40:21.584243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[16,  2,  2,  2],\n",
       "       [ 0, 27,  0,  0],\n",
       "       [ 0,  0, 28,  4],\n",
       "       [ 1,  0,  5, 13]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            batch[\"attention_mask\"].to(device)\n",
    "        )\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(batch[\"label\"].numpy())\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "confusion_matrix(all_labels, all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d73050-5974-4884-9c67-00ac0dcaffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d217a7-3825-4b76-8e87-0fe350d89d6e",
   "metadata": {},
   "source": [
    "### 9. Save the Trained Model and Prepare for Inference\n",
    "\n",
    "In this section, we save the fine-tuned model weights and prepare the files that will be\n",
    "used later inside the custom prediction container.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cab1801d-f185-47eb-bcd6-daee240259c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:42:06.270576Z",
     "iopub.status.busy": "2025-11-26T18:42:06.269096Z",
     "iopub.status.idle": "2025-11-26T18:42:06.606069Z",
     "shell.execute_reply": "2025-11-26T18:42:06.604712Z",
     "shell.execute_reply.started": "2025-11-26T18:42:06.270524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert_agnews_model/model.pt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = f\"{MODEL_DIR}/model.pt\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "MODEL_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b377b4f-2361-4696-af57-56ba4d735866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:43:42.444578Z",
     "iopub.status.busy": "2025-11-26T18:43:42.442901Z",
     "iopub.status.idle": "2025-11-26T18:43:42.450534Z",
     "shell.execute_reply": "2025-11-26T18:43:42.449074Z",
     "shell.execute_reply.started": "2025-11-26T18:43:42.444512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create the Inference Script (inference.py)\n",
    "#This script defines how Vertex AI will load and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5de09bf-95c5-4ad4-b322-8e3eddb6f570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T18:45:47.960740Z",
     "iopub.status.busy": "2025-11-26T18:45:47.960304Z",
     "iopub.status.idle": "2025-11-26T18:45:47.970706Z",
     "shell.execute_reply": "2025-11-26T18:45:47.969330Z",
     "shell.execute_reply.started": "2025-11-26T18:45:47.960705Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bert_agnews_model/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/inference.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LEN = 128\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(base_model_name)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        x = self.dropout(pooled)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        self.model = BertClassifier(MODEL_NAME)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict(self, instances):\n",
    "        texts = [inst[\"text\"] for inst in instances]\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].to(self.device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = probs.argmax(dim=1).cpu().tolist()\n",
    "            confs = probs.max(dim=1).values.cpu().tolist()\n",
    "\n",
    "        return [{\"label\": int(p), \"confidence\": float(c)} for p, c in zip(preds, confs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7e2f8-d355-4f68-b2ca-cd329a6ae850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast API Server(server.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f65e60-ffdc-42ad-8974-3a9aa640de06",
   "metadata": {},
   "source": [
    "### 11. Create the FastAPI Server (`server.py`)\n",
    "\n",
    "Vertex AI will not call `Predictor` directly. Instead, it sends HTTP requests\n",
    "to a web server that runs inside our custom container.\n",
    "\n",
    "We use **FastAPI** to create a small REST API that:\n",
    "\n",
    "- Loads the `Predictor` once at startup  \n",
    "- Exposes a `/predict` endpoint that accepts JSON input  \n",
    "- Returns predictions as JSON  \n",
    "\n",
    "This file will live in the same folder as `model.pt` and `inference.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a45b379-205d-42a0-a5d3-e1df0a1ff1ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:08:57.561107Z",
     "iopub.status.busy": "2025-11-27T13:08:57.560682Z",
     "iopub.status.idle": "2025-11-27T13:08:57.571135Z",
     "shell.execute_reply": "2025-11-27T13:08:57.569578Z",
     "shell.execute_reply.started": "2025-11-27T13:08:57.561075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert_agnews_model/server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert_agnews_model/server.py\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from inference import Predictor\n",
    "\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"model.pt\")\n",
    "\n",
    "app = FastAPI()\n",
    "predictor = Predictor(MODEL_PATH)\n",
    "\n",
    "class Instance(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    instances: List[Instance]\n",
    "\n",
    "\n",
    "# --- Health and metadata endpoints --- #\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    # Simple root handler so readiness probes get HTTP 200\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    # Explicit health endpoint (nice for debugging)\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/v1/endpoints/{endpoint_id}/deployedModels/{deployed_model_id}\")\n",
    "def model_info(endpoint_id: str, deployed_model_id: str):\n",
    "    # When Vertex calls this, just say \"yes, I'm here\"\n",
    "    return {\n",
    "        \"endpoint_id\": endpoint_id,\n",
    "        \"deployed_model_id\": deployed_model_id,\n",
    "        \"status\": \"ready\",\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Prediction endpoint --- #\n",
    "  \n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(request: PredictRequest):\n",
    "      \"\"\"\n",
    "    Main prediction endpoint.\n",
    "\n",
    "    Accepts a JSON body of the form:\n",
    "    {\n",
    "      \"instances\": [\n",
    "        {\"text\": \"some text\"},\n",
    "        {\"text\": \"another text\"}\n",
    "      ]\n",
    "    }\n",
    "\n",
    "    Returns:\n",
    "    {\n",
    "      \"predictions\": [\n",
    "        {\"label\": ..., \"confidence\": ...},\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "     # Convert Pydantic objects to simple dicts\n",
    "    instances = [{\"text\": inst.text} for inst in request.instances]\n",
    "    preds = predictor.predict(instances)\n",
    "    return {\"predictions\": preds}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07b10e-9f29-47d8-aac0-c76e0bc9e34d",
   "metadata": {},
   "source": [
    "### 12. Create `requirements.txt` for the Container\n",
    "\n",
    "The container needs to know which Python packages to install at build time.\n",
    "We list them in a `requirements.txt` file located in `bert_agnews_model/`.\n",
    "\n",
    "This is separate from the notebook environment: it is used when building\n",
    "the Docker image that will run on Vertex AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b259ac2-7c2e-46fe-b033-44e92cf72ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:13:35.456264Z",
     "iopub.status.busy": "2025-11-27T13:13:35.455429Z",
     "iopub.status.idle": "2025-11-27T13:13:35.463904Z",
     "shell.execute_reply": "2025-11-27T13:13:35.462469Z",
     "shell.execute_reply.started": "2025-11-27T13:13:35.456216Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bert_agnews_model/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/requirements.txt\n",
    "torch\n",
    "transformers\n",
    "fastapi\n",
    "uvicorn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a1ef3-49e8-4699-aa21-7c1630514c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T19:44:12.769311Z",
     "iopub.status.busy": "2025-11-27T19:44:12.768870Z",
     "iopub.status.idle": "2025-11-27T19:44:12.780784Z",
     "shell.execute_reply": "2025-11-27T19:44:12.778587Z",
     "shell.execute_reply.started": "2025-11-27T19:44:12.769273Z"
    },
    "tags": []
   },
   "source": [
    "### 13. Create a Dockerfile for the Custom Prediction Container\n",
    "\n",
    "The Dockerfile defines the environment that will run on Vertex AI.  \n",
    "It does the following:\n",
    "\n",
    "1. Starts from a small Python base image.  \n",
    "2. Sets `/app` as the working directory inside the container.  \n",
    "3. Copies everything from the local `bert_agnews_model/` folder into `/app/`.  \n",
    "4. Installs Python dependencies from `requirements.txt`.  \n",
    "5. Sets the `MODEL_PATH` environment variable that `Predictor` will read.  \n",
    "6. Launches the FastAPI app with Uvicorn on port `8080` (the default for Vertex).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f8dacb-cbd7-410c-92f2-25d5afc233d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:13:40.570641Z",
     "iopub.status.busy": "2025-11-27T13:13:40.570224Z",
     "iopub.status.idle": "2025-11-27T13:13:40.579362Z",
     "shell.execute_reply": "2025-11-27T13:13:40.577563Z",
     "shell.execute_reply.started": "2025-11-27T13:13:40.570605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY bert_agnews_model/ /app/\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "ENV MODEL_PATH=/app/model.pt\n",
    "\n",
    "CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ff664-c8c1-42c0-8a70-8c33b727d9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255a3b06-aa66-42a9-99e5-16dfabcd3f79",
   "metadata": {},
   "source": [
    "### 14. Build and Push the Image with Cloud Build\n",
    "\n",
    "We use **Cloud Build** and **Artifact Registry** to build and store the container image.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create an Artifact Registry repository (one time per project).  \n",
    "2. Configure Docker to authenticate with Artifact Registry.  \n",
    "3. Run `gcloud builds submit` to build the image from the Dockerfile and push it.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91b84a5-14bb-4334-b992-9f1cbd43612f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:13:44.769796Z",
     "iopub.status.busy": "2025-11-27T13:13:44.769128Z",
     "iopub.status.idle": "2025-11-27T13:13:46.181001Z",
     "shell.execute_reply": "2025-11-27T13:13:46.179600Z",
     "shell.execute_reply.started": "2025-11-27T13:13:44.769739Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create {IMAGE_REPO} \\\n",
    "    --repository-format=DOCKER \\\n",
    "    --location={REGION} \\\n",
    "    --description=\"Vertex MLOps workshop images\" || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755bc4d-ce82-462c-9372-80ee4c21510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14.2 Configure Docker to use gcloud credentials for Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7ad74e-1ad4-4bef-b475-8edae679958b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:13:55.089742Z",
     "iopub.status.busy": "2025-11-27T13:13:55.088380Z",
     "iopub.status.idle": "2025-11-27T13:13:56.064526Z",
     "shell.execute_reply": "2025-11-27T13:13:56.063025Z",
     "shell.execute_reply.started": "2025-11-27T13:13:55.089667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33181437-3856-4f9b-b56a-27b279c9fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure Cloud Build API is enabled !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11cf8927-8e12-49a9-a77d-ac14057ade1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T20:00:30.522257Z",
     "iopub.status.busy": "2025-11-27T20:00:30.521822Z",
     "iopub.status.idle": "2025-11-27T20:00:30.528317Z",
     "shell.execute_reply": "2025-11-27T20:00:30.526719Z",
     "shell.execute_reply.started": "2025-11-27T20:00:30.522220Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14.3 Build the Docker image from the current directory and push it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "762d7278-cd0c-4cf0-bcb6-1b0afa0586ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:14:01.575355Z",
     "iopub.status.busy": "2025-11-27T13:14:01.574881Z",
     "iopub.status.idle": "2025-11-27T13:25:38.677267Z",
     "shell.execute_reply": "2025-11-27T13:25:38.675538Z",
     "shell.execute_reply.started": "2025-11-27T13:14:01.575310Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 9 file(s) totalling 253.3 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://vast-collective-478617-j1_cloudbuild/source/1764249242.489282-ccbfc58cb54245e09c5644b4fabac7bd.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/vast-collective-478617-j1/locations/global/builds/fba4fb48-a8c3-4996-acac-5e086c8a7344].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/fba4fb48-a8c3-4996-acac-5e086c8a7344?project=60487384516 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fba4fb48-a8c3-4996-acac-5e086c8a7344\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://vast-collective-478617-j1_cloudbuild/source/1764249242.489282-ccbfc58cb54245e09c5644b4fabac7bd.tgz#1764249276565916\n",
      "Copying gs://vast-collective-478617-j1_cloudbuild/source/1764249242.489282-ccbfc58cb54245e09c5644b4fabac7bd.tgz#1764249276565916...\n",
      "\\ [1 files][233.5 MiB/233.5 MiB]                                                \n",
      "Operation completed over 1 objects/233.5 MiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/gcb-internal\n",
      "Sending build context to Docker daemon  265.6MB\n",
      "Step 1/6 : FROM python:3.11-slim\n",
      "3.11-slim: Pulling from library/python\n",
      "0e4bc2bd6656: Pulling fs layer\n",
      "22b63e76fde1: Pulling fs layer\n",
      "b3dd773c3296: Pulling fs layer\n",
      "1771569cc129: Pulling fs layer\n",
      "1771569cc129: Verifying Checksum\n",
      "1771569cc129: Download complete\n",
      "22b63e76fde1: Verifying Checksum\n",
      "22b63e76fde1: Download complete\n",
      "b3dd773c3296: Verifying Checksum\n",
      "b3dd773c3296: Download complete\n",
      "0e4bc2bd6656: Verifying Checksum\n",
      "0e4bc2bd6656: Download complete\n",
      "0e4bc2bd6656: Pull complete\n",
      "22b63e76fde1: Pull complete\n",
      "b3dd773c3296: Pull complete\n",
      "1771569cc129: Pull complete\n",
      "Digest: sha256:193fdd0bbcb3d2ae612bd6cc3548d2f7c78d65b549fcaa8af75624c47474444d\n",
      "Status: Downloaded newer image for python:3.11-slim\n",
      " ---> 040af88f5bce\n",
      "Step 2/6 : WORKDIR /app\n",
      " ---> Running in b3d7952af43f\n",
      "Removing intermediate container b3d7952af43f\n",
      " ---> d2c2d8394804\n",
      "Step 3/6 : COPY bert_agnews_model/ /app/\n",
      " ---> bbaa4edfdf30\n",
      "Step 4/6 : RUN pip install --no-cache-dir -r requirements.txt\n",
      " ---> Running in 53444ae1094c\n",
      "Collecting torch (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "      44.0/44.0 kB 148.6 MB/s eta 0:00:00\n",
      "Collecting fastapi (from -r requirements.txt (line 3))\n",
      "  Downloading fastapi-0.122.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting uvicorn (from -r requirements.txt (line 4))\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from torch->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch->-r requirements.txt (line 1))\n",
      "  Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "      62.1/62.1 kB 196.9 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "      40.5/40.5 kB 196.1 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "      57.7/57.7 kB 187.7 MB/s eta 0:00:00\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi->-r requirements.txt (line 3))\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi->-r requirements.txt (line 3))\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "      90.6/90.6 kB 228.7 MB/s eta 0:00:00\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi->-r requirements.txt (line 3))\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting click>=7.0 (from uvicorn->-r requirements.txt (line 4))\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 4))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 2))\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 3))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 3))\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->-r requirements.txt (line 3))\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.51.0,>=0.40.0->fastapi->-r requirements.txt (line 3))\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 1))\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi->-r requirements.txt (line 3))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "    899.8/899.8 MB 213.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "    594.3/594.3 MB 204.0 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "    10.2/10.2 MB 219.4 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "    88.0/88.0 MB 205.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "    954.8/954.8 kB 229.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "    706.8/706.8 MB 221.7 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "    193.1/193.1 MB 212.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "    1.2/1.2 MB 276.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "    63.6/63.6 MB 223.7 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "    267.5/267.5 MB 210.3 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "    288.2/288.2 MB 228.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "    287.2/287.2 MB 197.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "    322.3/322.3 MB 228.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "    39.3/39.3 MB 240.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "    124.7/124.7 MB 174.7 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "    90.0/90.0 kB 196.2 MB/s eta 0:00:00\n",
      "Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "    170.4/170.4 MB 211.6 MB/s eta 0:00:00\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "    12.0/12.0 MB 202.8 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.122.0-py3-none-any.whl (110 kB)\n",
      "    110.7/110.7 kB 236.1 MB/s eta 0:00:00\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "    68.1/68.1 kB 223.5 MB/s eta 0:00:00\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "    108.3/108.3 kB 234.7 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "    201.0/201.0 kB 184.6 MB/s eta 0:00:00\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "    566.1/566.1 kB 279.5 MB/s eta 0:00:00\n",
      "Downloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "    2.1/2.1 MB 256.0 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "    16.9/16.9 MB 232.3 MB/s eta 0:00:00\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "    66.5/66.5 kB 212.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "    463.6/463.6 kB 214.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "    2.1/2.1 MB 223.1 MB/s eta 0:00:00\n",
      "Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
      "    806.6/806.6 kB 289.7 MB/s eta 0:00:00\n",
      "Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "    800.4/800.4 kB 293.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "    507.2/507.2 kB 217.6 MB/s eta 0:00:00\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "    74.0/74.0 kB 226.1 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "    6.3/6.3 MB 230.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "    3.3/3.3 MB 225.8 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "    78.5/78.5 kB 198.7 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "    44.6/44.6 kB 151.7 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "    134.9/134.9 kB 237.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "    64.7/64.7 kB 212.0 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "    109.1/109.1 kB 110.4 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "    159.4/159.4 kB 50.6 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "    151.6/151.6 kB 178.7 MB/s eta 0:00:00\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "    3.3/3.3 MB 230.9 MB/s eta 0:00:00\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "    71.0/71.0 kB 221.7 MB/s eta 0:00:00\n",
      "Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "    536.2/536.2 kB 226.5 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "    129.8/129.8 kB 261.9 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, triton, tqdm, sympy, sniffio, safetensors, regex, pyyaml, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, h11, fsspec, filelock, click, charset_normalizer, certifi, annotated-types, annotated-doc, uvicorn, typing-inspection, requests, pydantic-core, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, anyio, starlette, pydantic, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, fastapi, transformers\n",
      "Successfully installed MarkupSafe-3.0.3 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.11.0 certifi-2025.11.12 charset_normalizer-3.4.4 click-8.3.1 fastapi-0.122.0 filelock-3.20.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6 numpy-2.3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pydantic-2.12.5 pydantic-core-2.41.5 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 sniffio-1.3.1 starlette-0.50.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 triton-3.5.1 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.5.0 uvicorn-0.38.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 53444ae1094c\n",
      " ---> 5c3fae5e0770\n",
      "Step 5/6 : ENV MODEL_PATH=/app/model.pt\n",
      " ---> Running in a50d81ac059a\n",
      "Removing intermediate container a50d81ac059a\n",
      " ---> d0584909091c\n",
      "Step 6/6 : CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
      " ---> Running in c5dca83163ad\n",
      "Removing intermediate container c5dca83163ad\n",
      " ---> d292fec20b0c\n",
      "Successfully built d292fec20b0c\n",
      "Successfully tagged us-central1-docker.pkg.dev/vast-collective-478617-j1/vertex-mlops/bert-agnews-workshop:v1\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/vast-collective-478617-j1/vertex-mlops/bert-agnews-workshop:v1\n",
      "The push refers to repository [us-central1-docker.pkg.dev/vast-collective-478617-j1/vertex-mlops/bert-agnews-workshop]\n",
      "51c0e83697d4: Preparing\n",
      "dff0e738ab14: Preparing\n",
      "c0b7b06f1d79: Preparing\n",
      "720ee7aae3ad: Preparing\n",
      "655ff69eb9c8: Preparing\n",
      "5c988eaa8862: Preparing\n",
      "70a290c5e58b: Preparing\n",
      "5c988eaa8862: Layer already exists\n",
      "720ee7aae3ad: Layer already exists\n",
      "655ff69eb9c8: Layer already exists\n",
      "c0b7b06f1d79: Pushed\n",
      "dff0e738ab14: Pushed\n",
      "70a290c5e58b: Pushed\n",
      "51c0e83697d4: Pushed\n",
      "v1: digest: sha256:b9817ec820ed81b5bfa0412a18e1dce97bcecba0d78131a4e12ac2c640a38717 size: 1793\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                                                     STATUS\n",
      "fba4fb48-a8c3-4996-acac-5e086c8a7344  2025-11-27T13:14:36+00:00  10M58S    gs://vast-collective-478617-j1_cloudbuild/source/1764249242.489282-ccbfc58cb54245e09c5644b4fabac7bd.tgz  us-central1-docker.pkg.dev/vast-collective-478617-j1/vertex-mlops/bert-agnews-workshop:v1  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --tag {IMAGE_URI}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e6b79-0761-42e4-a452-3864f8f3912d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d4d7966-ccb7-452c-a441-d02ecd87c3a3",
   "metadata": {},
   "source": [
    "### 15. (Optional) Upload Model Artifacts to Google Cloud Storage\n",
    "\n",
    "Even though the container already contains all model files, it is common MLOps\n",
    "practice to also store the raw artifacts in a GCS bucket.\n",
    "\n",
    "This gives you:\n",
    "\n",
    "- A backup of `model.pt` and your code.  \n",
    "- A central store for versioned models.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "844b3ff3-bddf-4d75-be56-6569d64314ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:26:05.978291Z",
     "iopub.status.busy": "2025-11-27T13:26:05.977781Z",
     "iopub.status.idle": "2025-11-27T13:26:12.170506Z",
     "shell.execute_reply": "2025-11-27T13:26:12.168965Z",
     "shell.execute_reply.started": "2025-11-27T13:26:05.978252Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://bert_agnews_model/inference.py [Content-Type=text/x-python]...\n",
      "Copying file://bert_agnews_model/model.pt [Content-Type=application/vnd.snesdev-page-table]...\n",
      "Copying file://bert_agnews_model/server.py [Content-Type=text/x-python]...      \n",
      "Copying file://bert_agnews_model/requirements.txt [Content-Type=text/plain]...  \n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [4/4 files][253.2 MiB/253.2 MiB] 100% Done                                    \n",
      "Operation completed over 4 objects/253.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r {MODEL_DIR} gs://{BUCKET_NAME}/models/{MODEL_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a767943-2e49-4f53-ad93-f5eead88626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e05ed88-c989-4965-b5b5-2a4d0cbbb2f1",
   "metadata": {},
   "source": [
    "### 16. Register the Model in Vertex AI\n",
    "\n",
    "Now we tell Vertex AI about our model by creating a **Model** resource.\n",
    "\n",
    "This resource points to:\n",
    "\n",
    "- The artifact directory in GCS (for bookkeeping).  \n",
    "- The custom container image in Artifact Registry.  \n",
    "- The HTTP route used for prediction (`/predict`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f95dd2d-566f-4fef-b923-8ab358a60583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:26:50.140181Z",
     "iopub.status.busy": "2025-11-27T13:26:50.139710Z",
     "iopub.status.idle": "2025-11-27T13:30:09.647045Z",
     "shell.execute_reply": "2025-11-27T13:30:09.645910Z",
     "shell.execute_reply.started": "2025-11-27T13:26:50.140144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/60487384516/locations/us-central1/models/1820517477201739776/operations/7482138470526222336\n",
      "Model created. Resource name: projects/60487384516/locations/us-central1/models/1820517477201739776@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/60487384516/locations/us-central1/models/1820517477201739776@1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/60487384516/locations/us-central1/models/1820517477201739776'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_model = aiplatform.Model.upload(\n",
    "    display_name=\"bert-agnews-workshop\",\n",
    "    artifact_uri=f\"gs://{BUCKET_NAME}/models/{MODEL_DIR}\",\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_predict_route=\"/predict\",\n",
    ")\n",
    "\n",
    "vertex_model.resource_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f25dbc-24fa-4227-b531-b995ecf4109c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60ed738e-96ff-4f64-9b9d-fd3916281933",
   "metadata": {},
   "source": [
    "\n",
    "#### 17. Create an Endpoint and Deploy the Model\n",
    "\n",
    "A **Model** is just a registered asset.  \n",
    "To actually serve traffic, we need a **Vertex AI Endpoint** and then deploy the model to it.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a new endpoint (think of it as a network address for predictions).  \n",
    "2. Deploy our custom container model to that endpoint.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56379522-fe85-4dfa-9b34-343bdb5a8af5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:32:42.083268Z",
     "iopub.status.busy": "2025-11-27T13:32:42.082831Z",
     "iopub.status.idle": "2025-11-27T13:32:44.481209Z",
     "shell.execute_reply": "2025-11-27T13:32:44.479886Z",
     "shell.execute_reply.started": "2025-11-27T13:32:42.083235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/60487384516/locations/us-central1/endpoints/1847415929663651840/operations/4498503717393268736\n",
      "Endpoint created. Resource name: projects/60487384516/locations/us-central1/endpoints/1847415929663651840\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/60487384516/locations/us-central1/endpoints/1847415929663651840')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/60487384516/locations/us-central1/endpoints/1847415929663651840'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"bert-agnews-endpoint\")\n",
    "endpoint.resource_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "522b17d0-0443-412a-9046-cb2516838e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:33:44.816041Z",
     "iopub.status.busy": "2025-11-27T13:33:44.815623Z",
     "iopub.status.idle": "2025-11-27T13:44:39.843418Z",
     "shell.execute_reply": "2025-11-27T13:44:39.841661Z",
     "shell.execute_reply.started": "2025-11-27T13:33:44.816004Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/60487384516/locations/us-central1/endpoints/1847415929663651840\n",
      "Deploy Endpoint model backing LRO: projects/60487384516/locations/us-central1/endpoints/1847415929663651840/operations/2838364304753819648\n",
      "Endpoint model deployed. Resource name: projects/60487384516/locations/us-central1/endpoints/1847415929663651840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f7ded132170> \n",
       "resource name: projects/60487384516/locations/us-central1/endpoints/1847415929663651840"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deploy_op = vertex_model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"n1-standard-2\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    traffic_percentage=100,\n",
    "    sync=True,\n",
    ")\n",
    "deploy_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b77c68-d08a-4187-92f6-75d064bd1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b92b2-205f-49a3-8d5f-b0f52ff21f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21dbf1c9-ab2b-4d9e-bb13-756823aa0b05",
   "metadata": {},
   "source": [
    "#### 18. Send Live Prediction Requests\n",
    "\n",
    "Once deployment completes successfully, the endpoint is ready to accept prediction requests.\n",
    "\n",
    "We use the Python SDK to send a list of instances. Each instance is a dictionary with a `text` field,\n",
    "matching the shape expected by our `Predictor.predict()` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2f0a0-bb14-4aed-a3d7-e5b15ab6d082",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1d5a2e7-369f-4b11-982e-50b891ccb95c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T13:51:44.276774Z",
     "iopub.status.busy": "2025-11-27T13:51:44.276320Z",
     "iopub.status.idle": "2025-11-27T13:51:44.748890Z",
     "shell.execute_reply": "2025-11-27T13:51:44.747514Z",
     "shell.execute_reply.started": "2025-11-27T13:51:44.276734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[{'label': 3.0, 'confidence': 0.8962261080741882}, {'label': 1.0, 'confidence': 0.7797536253929138}], deployed_model_id='9199005759671631872', metadata=None, model_version_id='1', model_resource_name='projects/60487384516/locations/us-central1/models/1820517477201739776', explanations=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_instances = [\n",
    "    {\"text\": \"Apple releases a new AI powered smartphone with advanced features.\"},\n",
    "    {\"text\": \"The local team won the national championship in a thrilling final.\"},\n",
    "]\n",
    "\n",
    "endpoint.predict(sample_instances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab245cd4-a2d2-4f53-95b8-122a8c83285d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e41ff9b-c205-48a3-81d9-01b56ee2b88a",
   "metadata": {},
   "source": [
    "If everything is wired correctly, you should see a response similar to:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predictions\": [\n",
    "    {\"label\": 3, \"confidence\": 0.87},\n",
    "    {\"label\": 1, \"confidence\": 0.91}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d192fe-5389-4954-aa4f-fbd8b769cb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
